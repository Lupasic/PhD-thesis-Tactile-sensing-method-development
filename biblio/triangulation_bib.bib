
@article{aurenhammerVoronoiDiagramsSurvey1991,
  title = {Voronoi Diagrams—a Survey of a Fundamental Geometric Data Structure},
  author = {Aurenhammer, Franz},
  date = {1991-09},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {23},
  number = {3},
  pages = {345--405},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/116873.116880},
  url = {https://dl.acm.org/doi/10.1145/116873.116880},
  urldate = {2022-08-26},
  langid = {english},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\ME9JSBIA\\Aurenhammer_1991_Voronoi diagrams—a survey of a fundamental geometric data structure.pdf}
}

@inproceedings{ebertInterpolationExtrapolationComparison2014,
  title = {Interpolation and Extrapolation: {{Comparison}} of Definitions and Survey of Algorithms for Convex and Concave Hulls},
  shorttitle = {Interpolation and Extrapolation},
  booktitle = {2014 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  author = {Ebert, Tobias and Belz, Julian and Nelles, Oliver},
  date = {2014-12},
  pages = {310--314},
  publisher = {{IEEE}},
  location = {{Orlando, FL, USA}},
  doi = {10.1109/CIDM.2014.7008683},
  url = {http://ieeexplore.ieee.org/document/7008683/},
  urldate = {2022-08-26},
  eventtitle = {2014 {{IEEE Symposium}} on {{Computational Intelligence}} and {{Data Mining}} ({{CIDM}})},
  isbn = {978-1-4799-4518-4},
  langid = {english},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\33U5N225\\Ebert et al_2014_Interpolation and extrapolation.pdf}
}

@article{kumarSurfaceTriangulationSurvey,
  title = {Surface {{Triangulation}}: {{A Survey}}},
  author = {Kumar, Subodh},
  pages = {19},
  abstract = {This paper presents a brief survey of some problems and solutions related to the triangulation of surfaces. A surface (a two dimensional manifold, in the context of this paper) can be represented as a three dimensional function on a planar disk. In that sense, the triangulation of the disk induces a triangulation of the surface. Hence the emphasis of this paper is on triangulation on a plane. Apart from the issues in triangulation, this survey talks about the known upper and lower bounds on various triangulation problems. It is intended as a broad compilation of known results rather than an intensive treatise, and the details of most algorithms are skipped.},
  langid = {english},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\9B7BIYZF\\Kumar - Surface Triangulation A Survey.pdf}
}

@article{amentaPowerCrust2001,
  title = {The Power Crust},
  author = {Amenta, Nina and Choi, Sunghee and Kolluri, Ravi Krishna},
  date = {2001-05-01},
  pages = {249--266},
  doi = {10.1145/376957.376986},
  abstract = {The power crust is a construction which takes a sample of points from the surface of a three-dimensional object and produces a surface mesh and an approximate medial axis. The approach is to first approximate the medial axis transform (MAT) of the object. We then use an inverse transform to produce the surface representation from the MAT.  This idea leads to a simple algorithm with theoretical guarantees comparable to those of other surface reconstruction and medial axis approximation algorithms. It also comes with a guarantee that does not depend in any way on the quality of the input point sample. Any input gives an output surface which is the `watertight' boundary of a three-dimensional polyhedral solid: the solid described by the approximate MAT. This unconditional guarantee makes the algorithm quite robust and eliminates the polygonalization, hole-filling or manifold extraction post-processing steps required in previous surface reconstruction algorithms.  In this paper, we use the theory to develop a power crust implementation which is indeed robust for realistic and even difficult samples. We describe the careful design of a key subroutine which labels parts of the MAT as inside or outside of the object, easy in theory but non-trivial in practice. We find that we can handle areas in which the input sampling is scanty or noisy by simply discarding the unreliable parts of the MAT approximation. We demonstrate good empirical results on inputs including models with sharp corners, sparse and unevenly distributed point samples, holes, and noise, both natural and synthetic.  We also demonstrate some simple extensions: intentionally leaving holes where there is no data, producing approximate offset surfaces, and simplifying the approximate MAT in a principled way to preserve stable features.},
  langid = {english},
  annotation = {MAG ID: 2020065037},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\QRI695QK\\Amenta et al_2001_The power crust.pdf}
}

@article{amentaPowerCrustUnions2001,
  title = {The Power Crust, Unions of Balls, and the Medial Axis Transform},
  author = {Amenta, Nina and Choi, Sunghee and Kolluri, Ravi Krishna},
  date = {2001-07-01},
  journaltitle = {Computational Geometry: Theory and Applications},
  volume = {19},
  number = {2},
  pages = {127--153},
  doi = {10.1016/s0925-7721(01)00017-7},
  abstract = {The medial axis transform (or MAT) is a representation of an object as an infinite union of balls. We consider approximating the MAT of a three-dimensional object, and its complement, with a finite union of balls. Using this approximate MAT we define a new piecewise-linear approximation to the object surface, which we call the power crust. We assume that we are given as input a sufficiently dense sample of points from the object surface. We select a subset of the Voronoi balls of the sample, the polar balls, as the union of balls representation. We bound the geometric error of the union, and of the corresponding power crust, and show that both representations are topologically correct as well. Thus, our results provide a new algorithm for surface reconstruction from sample points. By construction, the power crust is always the boundary of a polyhedral solid, so we avoid the polygonization, hole-filling or manifold extraction steps used in previous algorithms. The union of balls representation and the power crust have corresponding piecewise-linear dual representations, which in some sense approximate the medial axis. We show a geometric relationship between these duals and the medial axis by proving that, as the sampling density goes to infinity, the set of poles, the centers of the polar balls, converges to the medial axis.},
  langid = {english},
  annotation = {MAG ID: 2088352159},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\NQCQIP56\\Amenta et al_2001_The power crust, unions of balls, and the medial axis transform.pdf}
}

@article{avisHowGoodAre1997,
  title = {How Good Are Convex Hull Algorithms},
  author = {Avis, David and Bremner, David and Seidel, Raimund},
  date = {1997-04-01},
  journaltitle = {Computational Geometry: Theory and Applications},
  volume = {7},
  number = {5},
  pages = {265--301},
  doi = {10.1016/s0925-7721(96)00023-5},
  abstract = {Abstract   A convex polytope P can be specified in two ways: as the convex hull of the vertex set V of P, or as the intersection of the set H of its facet-inducing halfspaces. The vertex enumeration problem is to compute V from H{$>$}. The facet enumeration problem is to compute H from V. These two problems are essentially equivalent under point/hyperplane duality. They are among the central computational problems in the theory of polytopes. It is open whether they can be solved in time polynomial in |H| + |V| and the dimension. In this paper we consider the main known classes of algorithms for solving these problems. We argue that they all have at least one of two weaknesses: inability to deal well with “degeneracies”, or, inability to control the sizes of intermediate results. We then introduce families of polytopes that exercise those weaknesses. Roughly speaking, fat-lattice or intricate polytopes cause algorithms with bad degeneracy handling to perform badly; dwarfed polytopes cause algorithms with bad intermediate size control to perform badly. We also present computational experience with trying to solve these problem on these hard polytopes, using various implementations of the main algorithms.},
  langid = {english},
  annotation = {MAG ID: 2218652040},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\XB58SQBL\\Avis et al_1997_How good are convex hull algorithms.pdf}
}

@article{baranyiEffectsParameterizationPerformance1996,
  title = {Effects of Parameterization on the Performance of Empirical Models Used in `predictive Microbiology'},
  author = {Baranyi, József and Ross, Tom and McMeekin, Tom A. and Roberts, T.A.},
  date = {1996-02-01},
  journaltitle = {Food Microbiology},
  volume = {13},
  number = {1},
  pages = {83--91},
  doi = {10.1006/fmic.1996.0011},
  abstract = {Empirical models fitted to measured data should be used only in the region within which measurements were made, the so-called interpolation region. In cases in which many variables are involved, the determination of the interpolation region is not self-evident and the region is sometimes unexpectedly small. A definition of the interpolation region is presented, to enable some consequences of the use of models with high numbers of parameters to be exemplified. In particular, unreliability close to the boundary of the interpolation region is highlighted by comparison of the predictions of models with different numbers of parameters.},
  langid = {english},
  annotation = {MAG ID: 2076224062},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\6YEEVWME\\Baranyi et al_1996_Effects of parameterization on the performance of empirical models used in.pdf}
}

@article{barberQuickhullAlgorithmConvex1996,
  title = {The Quickhull Algorithm for Convex Hulls},
  author = {Barber, C. Bradford and Dobkin, David P. and Huhdanpaa, Hannu},
  date = {1996-12-01},
  journaltitle = {ACM Transactions on Mathematical Software},
  volume = {22},
  number = {4},
  pages = {469--483},
  doi = {10.1145/235815.235821},
  abstract = {The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it used less memory. computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serous errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.},
  langid = {english},
  annotation = {MAG ID: 2153504150},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\VZFQ2NJ8\\Barber et al_1996_The quickhull algorithm for convex hulls.pdf}
}

@article{barnardExtrapolationInterpolationNeural1992,
  title = {Extrapolation and Interpolation in Neural Network Classifiers},
  author = {Barnard, Etienne and Wessels, Lodewyk F. A.},
  date = {1992-10-01},
  journaltitle = {IEEE Control Systems Magazine},
  volume = {12},
  number = {5},
  pages = {50--53},
  doi = {10.1109/37.158898},
  abstract = {Multilayer perceptrons have recently been shown by M.A. Kramer and J.A. Leonard (1990, 1991) to give anomalous behavior in a diagnosis application. It is shown that this unsatisfactory behavior indicates a mismatch of application and technique, rather than any deficiency in the multilayer perceptron. It is further argued that this mismatch is indicative of the differences between applications that require interpolation and those which require extrapolation in addition. Simulation results to indicate a satisfactory performance of multilayer perceptrons in suitable applications are presented. {$>$}},
  langid = {english},
  annotation = {MAG ID: 1965559057},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\YD2IB5A4\\Barnard_Wessels_1992_Extrapolation and interpolation in neural network classifiers.pdf}
}

@article{bobachNaturalNeighborExtrapolation2009,
  title = {Natural Neighbor Extrapolation Using Ghost Points},
  author = {Bobach, Tom and Farin, Gerald and Hansford, Dianne and Umlauf, Georg},
  date = {2009-05-01},
  journaltitle = {Computer-aided Design},
  volume = {41},
  number = {5},
  pages = {350--365},
  doi = {10.1016/j.cad.2008.08.007},
  abstract = {Among locally supported scattered data schemes, natural neighbor interpolation has some unique features that makes it interesting for a range of applications. However, its restriction to the convex hull of the data sites is a limitation that has not yet been satisfyingly overcome. We use this setting to discuss some aspects of scattered data extrapolation in general, compare existing methods, and propose a framework for the extrapolation of natural neighbor interpolants on the basis of dynamic ghost points.},
  langid = {english},
  annotation = {MAG ID: 2166697565},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\WF9E6XG5\\Bobach et al_2009_Natural neighbor extrapolation using ghost points.pdf}
}

@article{brodalDynamicPlanarConvex2002,
  title = {Dynamic Planar Convex Hull},
  author = {Brodal, Gerth Stølting and Jacob, Riko},
  date = {2002-11-16},
  pages = {617--626},
  doi = {10.1109/sfcs.2002.1181985},
  abstract = {In this paper we determine the computational complexity of the dynamic convex hull problem in the planar case. We present a data structure that maintains a finite set of n points in the plane under insertion and deletion of points in amortized O(log n) time per operation. The space usage of the data structure is O(n). The data structure supports extreme point queries in a given direction, tangent queries through a given point, and queries for the neighboring points on the convex hull in O(log n) time. The extreme point queries can be used to decide whether or not a given line intersects the convex hull, and the tangent queries to determine whether a given point is inside the convex hull. We give a lower bound on the amortized asymptotic time complexity that matches the performance of this data structure.},
  langid = {english},
  annotation = {MAG ID: 2096606628},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\BAKCWQ83\\Brodal_Jacob_2002_Dynamic planar convex hull.pdf}
}

@article{brooksCharacterizingDomainRegression1988,
  title = {Characterizing the {{Domain}} of a {{Regression Model}}},
  author = {Brooks, Daniel G. and Carroll, Steven S. and Verdini, William A.},
  date = {1988-08-01},
  journaltitle = {The American Statistician},
  volume = {42},
  number = {3},
  pages = {187--190},
  doi = {10.1080/00031305.1988.10475559},
  abstract = {Abstract Several frequently used estimates of the domain of a regression model are discussed and compared with the convex hull of the predictor-variable values. A simple procedure for identifying new prediction values as interpolation or extrapolation points is presented using the convex hull as the model domain estimate and is illustrated by examples.},
  langid = {english},
  annotation = {MAG ID: 2069264681},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\2R6TVVRC\\Brooks et al_1988_Characterizing the Domain of a Regression Model.pdf}
}

@article{chadnovConvexHullAlgorithms2004,
  title = {Convex Hull Algorithms Review},
  author = {Chadnov, R.V. and Skvortsov, A.V.},
  date = {2004-06-26},
  volume = {2},
  pages = {112--115},
  doi = {10.1109/korus.2004.1555560},
  abstract = {In this paper, various convex hull algorithms are being analyzed. Various algorithms are compared},
  langid = {english},
  annotation = {MAG ID: 1516603892},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\RYVD9Z3B\\Chadnov_Skvortsov_2004_Convex hull algorithms review.pdf}
}

@article{chanOptimalOutputsensitiveConvex1996,
  title = {Optimal Output-Sensitive Convex Hull Algorithms in Two and Three Dimensions},
  author = {Chan, Timothy M.},
  date = {1996-04-01},
  journaltitle = {Discrete and Computational Geometry},
  volume = {16},
  number = {4},
  pages = {361--368},
  doi = {10.1007/bf02712873},
  abstract = {We present simple output-sensitive algorithms that construct the convex hull of a set ofn points in two or three dimensions in worst-case optimalO (n logh) time andO (n) space, whereh denotes the number of vertices of the convex hull.},
  langid = {english},
  annotation = {MAG ID: 2049074379},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\ZYD4LEH8\\Chan_1996_Optimal output-sensitive convex hull algorithms in two and three dimensions.pdf}
}

@article{chauBorderSamplesDetection2011,
  title = {Border Samples Detection for Data Mining Applications Using Non Convex Hulls},
  author = {Chau, Asdrúbal López and Li, Xiaoou and Yu, Wen and Cervantes, Jair and Mejia-Alvarez, Pedro},
  date = {2011-11-26},
  pages = {261--272},
  doi = {10.1007/978-3-642-25330-0_23},
  abstract = {Border points are those instances located at the outer margin of dense clusters of samples. The detection is important in many areas such as data mining, image processing, robotics, geographic information systems and pattern recognition. In this paper we propose a novel method to detect border samples. The proposed method makes use of a discretization and works on partitions of the set of points. Then the border samples are detected by applying an algorithm similar to the presented in reference [8] on the sides of convex hulls. We apply the novel algorithm on classification task of data mining; experimental results show the effectiveness of our method.},
  langid = {english},
  annotation = {MAG ID: 163519181}
}

@article{cintraSpeculativeParallelizationRandomized2004,
  title = {Speculative {{Parallelization}} of a {{Randomized Incremental Convex Hull Algorithm}}},
  author = {Cintra, Marcelo and Llanos, Diego R. and Palop, Belén},
  date = {2004-05-14},
  pages = {188--197},
  doi = {10.1007/978-3-540-24767-8_20},
  abstract = {Finding the fastest algorithm to solve a problem is one of the main issues in Computational Geometry. Focusing only on worst case analysis or asymptotic computations leads to the development of complex data structures or hard to implement algorithms. Randomized algorithms appear in this scenario as a very useful tool in order to obtain easier implementations within a good expected time bound. However, parallel implementations of these algorithms are hard to develop and require an in-depth understanding of the language, the compiler and the underlying parallel computer architecture. In this paper we show how we can use speculative parallelization techniques to execute in parallel iterative algorithms such as randomized incremental constructions. In this paper we focus on the convex hull problem, and show that, using our speculative parallelization engine, the sequential algorithm can be automatically executed in parallel, obtaining speedups with as little as four processors, and reaching 5.15x speedup with 28 processors.},
  langid = {english},
  annotation = {MAG ID: 1521860401},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\DJXFJ789\\Cintra et al_2004_Speculative Parallelization of a Randomized Incremental Convex Hull Algorithm.pdf}
}

@article{clarksonApplicationsRandomSampling1988,
  title = {Applications of Random Sampling in Computational Geometry, {{II}}},
  author = {Clarkson, Kenneth L.},
  date = {1988-01-06},
  volume = {4},
  number = {5},
  pages = {1--11},
  doi = {10.1145/73393.73394},
  abstract = {Random sampling is used for several new geometric algorithms. The algorithms are “Las Vegas,” and their expected bounds are with respect to the random behavior of the algorithms. One algorithm reports all the intersecting pairs of a set of line segments in the plane, and requires  O ( A  +  n  log  n ) expected time, where  A  is the size of the answer, the number of intersecting pairs reported. The algorithm requires  O ( n ) space in the worst case. Another algorithm computes the convex hull of a point set in  E  3  in  O ( n  log  A ) expected time, where  n  is the number of points and  A  is the number of points on the surface of the hull. A simple Las Vegas algorithm triangulates simple polygons in  O ( n  log log  n ) expected time. Algorithms for half-space range reporting are also given. In addition, this paper gives asymptotically tight bounds for a combinatorial quantity of interest in discrete and computational geometry, related to halfspace partitions of point sets.},
  langid = {english},
  annotation = {MAG ID: 2058432138},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\K78GH3FG\\Clarkson_1988_Applications of random sampling in computational geometry, II.pdf}
}

@article{cookDetectionInfluentialObservation2000,
  title = {Detection of Influential Observation in Linear Regression},
  author = {Cook, R. Dennis},
  date = {2000-02-01},
  journaltitle = {Technometrics},
  volume = {42},
  number = {1},
  pages = {65--68},
  doi = {10.2307/1271434},
  abstract = {A new measure based on confidence ellipsoids is developed for judging the contribution of each data point to the determination of the least squares estimate of the parameter vector in full rank linear regression models. It is shown that the measure combines information from the studentized residuals and the variances of the residuals and predicted values. Two examples are presented.},
  langid = {english},
  annotation = {MAG ID: 1998613841},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\FFYN7IRV\\Cook_2000_Detection of influential observation in linear regression.pdf}
}

@article{duckhamEfficientGenerationSimple2008,
  title = {Efficient Generation of Simple Polygons for Characterizing the Shape of a Set of Points in the Plane},
  author = {Duckham, Matt and Kulik, Lars and Worboys, Michael F. and Galton, Antony},
  date = {2008-10-01},
  journaltitle = {Pattern Recognition},
  volume = {41},
  number = {10},
  pages = {3224--3236},
  doi = {10.1016/j.patcog.2008.03.023},
  abstract = {This paper presents a simple, flexible, and efficient algorithm for constructing a possibly non-convex, simple polygon that characterizes the shape of a set of input points in the plane, termed a characteristic shape. The algorithm is based on the Delaunay triangulation of the points. The shape produced by the algorithm is controlled by a single normalized parameter, which can be used to generate a finite, totally ordered family of related characteristic shapes, varying between the convex hull at one extreme and a uniquely defined shape with minimum area. An optimal O(nlogn) algorithm for computing the shapes is presented. Characteristic shapes possess a number of desirable properties, and the paper includes an empirical investigation of the shapes produced by the algorithm. This investigation provides experimental evidence that with appropriate parameterization the algorithm is able to accurately characterize the shape of a wide range of different point distributions and densities. The experiments detail the effects of changing parameter values and provide an indication of some ''good'' parameter values to use in certain circumstances.},
  langid = {english},
  annotation = {MAG ID: 2164799852},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\CPZD4PVS\\Duckham et al_2008_Efficient generation of simple polygons for characterizing the shape of a set.pdf}
}

@article{edelsbrunnerShapeSetPoints1983,
  title = {On the Shape of a Set of Points in the Plane},
  author = {Edelsbrunner, Herbert and {David G. Kirkpatrick} and Kirkpatrick, David G. and Seidel, Raimund},
  date = {1983-07-01},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {29},
  number = {4},
  pages = {551--559},
  doi = {10.1109/tit.1983.1056714},
  abstract = {A generalization of the convex hull of a finite set of points in the plane is introduced and analyzed. This generalization leads to a family of straight-line graphs, " \textbackslash alpha -shapes," which seem to capture the intuitive notions of "fine shape" and "crude shape" of point sets. It is shown that a-shapes are subgraphs of the closest point or furthest point Delaunay triangulation. Relying on this result an optimal O(n \textbackslash log n) algorithm that constructs \textbackslash alpha -shapes is developed.},
  langid = {english},
  annotation = {MAG ID: 2151631165},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\7GDAHKCG\\Edelsbrunner et al_1983_On the shape of a set of points in the plane.pdf}
}

@article{edelsbrunnerThreedimensionalAlphaShapes1992,
  title = {Three-Dimensional Alpha Shapes},
  author = {Edelsbrunner, Herbert and Mucke, Ernst Peter},
  date = {1992-12-01},
  pages = {75--82},
  doi = {10.1145/147130.147153},
  abstract = {Frequently, data in scientific computing is in its abstract form a finite point set in space, and it is sometimes useful or required to compute what one might call the “shape” of the set. For that purpose, this article introduces the formal notion of the family of α-shapes of a finite point set in R 3 . Each shape is a well-defined polytope, derived from the Delaunay triangulation of the point set, with a parameter α e R controlling the desired level of detail. An algorithm is presented that constructs the entire family of shapes for a given set of size  n  in time  0(n 2 ) , worst case. A robust implementation of the algorithm is discussed, and several applications in the area of scientific computing  are mentioned.},
  langid = {english},
  annotation = {MAG ID: 1989072478}
}

@article{ertozNewSharedNearest2002,
  title = {A {{New Shared Nearest Neighbor Clustering Algorithm}} and Its {{Applications}}},
  author = {Ertoz, Levent and Steinbach, Michael and Kumar, Vipin},
  date = {2002-01-01},
  abstract = {Clustering depends critically on density and distance (similarity), but these concepts become increasingly more difficult to define as dimensionality increases. In this paper we offer definitions of density and similarity that work well for high dimensional data (actually, for data of any dimensionality). In particular, we use a similarity measure that is based on the number of neighbors that two points share, and define the density of a point as the sum of the similarities of a point’s nearest neighbors. We then present a new clustering algorithm that is based on these ideas. This algorithm eliminates noise (low density points) and builds clusters by associating non-noise points with representative or core points (high density points). This approach handles many problems that traditionally plague clustering algorithms, e.g., finding clusters in the presence of noise and outliers and finding clusters in data that has clusters of different shapes, sizes, and density. We have used our clustering algorithm on a variety of high and low dimensional data sets with good results, but in this paper, we present only a couple of examples involving high dimensional data sets: word clustering and time series derived from NASA Earth science data.},
  langid = {english},
  annotation = {MAG ID: 9613553}
}

@article{friedmanDiagnosticsExtrapolationMachine2004,
  title = {Diagnostics and Extrapolation in Machine Learning},
  author = {Friedman, Jerome and Hooker, Giles},
  date = {2004-01-01},
  abstract = {The subject of this thesis is the interaction between the problems of diagnostics and extrapolation in Machine Learning.  I present a suite of tools for understanding high dimensional prediction functions that are based on the Functional ANOVA decomposition and argue that these are optimal in an idealized setting. I then show that they can be distorted to an arbitrary extent if the predictor space contains large regions of extrapolation.  This thesis gives a criterion of extrapolation and details tree-based methods to evaluate it. This methodology provides a comprehensible representation of the distribution of training data and a diagnostic for functional behavior in regions of low data density. I then discuss the issue of making predictions at points of extrapolation. I suggest a strategy for stabilizing a general learning algorithm away from training data that is motivated by a Bayesian heuristic not unlike ridge regression and which bears some resemblance to Kriging.  Finally, I advocate a modification to the Functional ANOVA that uses this estimate to avoid the effects of bad extrapolation while retaining many of the useful properties of the decomposition.  All the ideas in this work are designed to be fully general and compatible with any machine learning algorithm.},
  langid = {english},
  annotation = {MAG ID: 2515381793}
}

@article{galtonWhatRegionOccupied2006,
  title = {What {{Is}} the {{Region Occupied}} by a {{Set}} of {{Points}}},
  author = {Galton, Antony and Duckham, Matt},
  date = {2006-01-01},
  journaltitle = {Lecture Notes in Computer Science},
  pages = {81--98},
  abstract = {There are many situations in GiScience where it would be useful to be able to assign a region to characterize the space occupied by a set of points. Such a region should represent the location or configuration of the points as an aggregate, abstracting away from the individual points themselves. In this paper, we call such a region a 'footprint' for the points. We investigate and compare a number of methods for producing such footprints, with respect to nine general criteria. The discussion identifies a number of potential choices and avenues for further research. Finally, we contrast the related research already conducted in this area, highlighting differences between these existing constructs and our footprints'.},
  langid = {english},
  annotation = {MAG ID: 2612140825}
}

@article{gartnerFastRobustSmallest1999,
  title = {Fast and {{Robust Smallest Enclosing Balls}}},
  author = {Gärtner, Bernd},
  date = {1999-07-16},
  pages = {325--338},
  doi = {10.1007/3-540-48481-7_29},
  abstract = {I describe a C++ program for computing the smallest enclosing ball of a point set in d-dimensional space, using floating-point arithmetic only. The program is very fast for d ? 20, robust and simple (about 300 lines of code, excluding prototype definitions). Its new features are a pivoting approach resembling the simplex method for linear programming, and a robust update scheme for intermediate solutions. The program with complete documentation following the literate programming paradigm [3] is available on the Web.},
  langid = {english},
  annotation = {MAG ID: 1606594487},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\6CMD5MZQ\\Gärtner_1999_Fast and Robust Smallest Enclosing Balls.pdf}
}

@article{gluhovskyConstrainedMultivariateExtrapolation2007,
  title = {Constrained {{Multivariate Extrapolation Models With Application}} to {{Computer Cache Rates}}},
  author = {Gluhovsky, Ilya and Gluhovsky, Ilya and Vengerov, David and Vengerov, David},
  date = {2007-05-01},
  journaltitle = {Technometrics},
  volume = {49},
  number = {2},
  pages = {129--137},
  doi = {10.1198/004017006000000426},
  abstract = {In this article we propose an approach to building multivariate regression models for prediction beyond the range of the data. The extrapolation model attempts to accurately estimate the high-level trend of the data, which can be extended in a natural way. The constraints of monotonicity and convexity/concavity play an important role in restricting the choice of the high level-trend, which otherwise would remain rather arbitrary. Our extrapolation model incorporates these constraints in multiple dimensions. We describe the trend as a nonnegative linear combination of twice-integrated multivariate B-splines and their variations. The specific basis functions in our approach are chosen so that any such combination is a plausible a priori model. As a result, basis function coefficients can be optimized to best fit the data without losing control over the high-level trend of the extrapolation model. Our approach also allows the use of standard model selection techniques. We illustrate this by applying cross-va...},
  langid = {english},
  annotation = {MAG ID: 2069313755},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\ZM76868K\\Gluhovsky et al_2007_Constrained Multivariate Extrapolation Models With Application to Computer.pdf}
}

@article{gonzalez-escribanoParallelizationAlternativesTheir2006,
  title = {Parallelization Alternatives and Their Performance for the Convex Hull Problem},
  author = {Gonzalez-Escribano, Arturo and Llanos, Diego R. and Orden, David and Palop, Belén},
  date = {2006-07-01},
  journaltitle = {Applied Mathematical Modelling},
  volume = {30},
  number = {7},
  pages = {563--577},
  doi = {10.1016/j.apm.2005.05.022},
  abstract = {High performance machines have become available nowadays to an increasing number of researchers. Most of us might have both an access to a supercomputing center and an algorithm that could benefit from these high performance machines. The aim of the present work is to revisit all existing parallelization alternatives, including emerging technologies like software-only speculative parallelization, to solve on different architectures the same representative problem: The computation of the convex hull of a point set.},
  langid = {english},
  annotation = {MAG ID: 2088515261},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\NWD8GYYI\\Gonzalez-Escribano et al_2006_Parallelization alternatives and their performance for the convex hull problem.pdf}
}

@article{grahamEfficientAlgorithDetermining1972,
  title = {An Efficient Algorith for Determining the Convex Hull of a Finite Planar Set},
  author = {Graham, Ron},
  date = {1972-06-01},
  journaltitle = {Information Processing Letters},
  volume = {1},
  number = {4},
  pages = {132--133},
  doi = {10.1016/0020-0190(72)90045-2},
  langid = {english},
  annotation = {MAG ID: 2025299767},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\CYQSGLAS\\Graham_1972_An efficient algorith for determining the convex hull of a finite planar set.pdf}
}

@article{haffnerEscapingConvexHull2001,
  title = {Escaping the {{Convex Hull}} with {{Extrapolated Vector Machines}}},
  author = {Haffner, Patrick},
  date = {2001-01-03},
  volume = {14},
  pages = {753--760},
  abstract = {Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity.},
  langid = {english},
  annotation = {MAG ID: 2138603399}
}

@article{hookerDiagnosingExtrapolationTreebased2004,
  title = {Diagnosing Extrapolation: Tree-Based Density Estimation},
  author = {Hooker, Giles},
  date = {2004-08-22},
  pages = {569--574},
  doi = {10.1145/1014052.1014121},
  abstract = {There has historically been very little concern with extrapolation in Machine Learning, yet extrapolation can be critical to diagnose. Predictor functions are almost always learned on a set of highly correlated data comprising a very small segment of predictor space. Moreover, flexible predictors, by their very nature, are not controlled at points of extrapolation. This becomes a problem for diagnostic tools that require evaluation on a product distribution. It is also an issue when we are trying to optimize a response over some variable in the input space. Finally, it can be a problem in non-static systems in which the underlying predictor distribution gradually drifts with time or when typographical errors misrecord the values of some predictors.We present a diagnosis for extrapolation as a statistical test for a point originating from the data distribution as opposed to a null hypothesis uniform distribution. This allows us to employ general classification methods for estimating such a test statistic. Further, we observe that CART can be modified to accept an exact distribution as an argument, providing a better classification tool which becomes our extrapolation-detection procedure. We explore some of the advantages of this approach and present examples of its practical application.},
  langid = {english},
  annotation = {MAG ID: 2018447176},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\ASWL4PBC\\Hooker_2004_Diagnosing extrapolation.pdf}
}

@article{j.a.leonardUsingRadialBasis1992,
  title = {Using Radial Basis Functions to Approximate a Function and Its Error Bounds},
  author = {{J.A. Leonard} and Leonard, James A. and Kramer, Mark A. and Ungar, Lyle H.},
  date = {1992-07-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {4},
  eprint = {18276463},
  eprinttype = {pmid},
  pages = {624--627},
  doi = {10.1109/72.143377},
  abstract = {A novel network called the validity index network (VI net) is presented. The VI net, derived from radial basis function networks, fits functions and calculates confidence intervals for its predictions, indicating local regions of poor fit and extrapolation. {$>$}},
  langid = {english},
  annotation = {MAG ID: 2106622844},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\NKBHHYL7\\J.A. Leonard et al_1992_Using radial basis functions to approximate a function and its error bounds.pdf}
}

@article{jarvisIdentificationConvexHull1973,
  title = {On the Identification of the Convex Hull of a Finite Set of Points in the Plane.},
  author = {Jarvis, Ray and Jarvis, Ray A.},
  date = {1973-03-01},
  journaltitle = {Information Processing Letters},
  volume = {2},
  number = {1},
  pages = {18--21},
  doi = {10.1016/0020-0190(73)90020-3},
  langid = {english},
  annotation = {MAG ID: 2000358470},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\KVVNUMHB\\Jarvis_Jarvis_1973_On the identification of the convex hull of a finite set of points in the plane.pdf}
}

@article{karlssonScanlineAlgorithmsGrid1988,
  title = {Scanline Algorithms on a Grid},
  author = {Karlsson, Rolf G. and Overmars, Mark H.},
  date = {1988-06-01},
  journaltitle = {Bit Numerical Mathematics},
  volume = {28},
  number = {2},
  pages = {227--241},
  doi = {10.1007/bf01934088},
  abstract = {A number of important problems in computational geometry are solved efficiently on 2- or 3-dimensional grids by means of scanline techniques. In the time complexity of solutions to the maximal elements and closure problems, a factor logn is substituted by loglogn, wheren is the number of elements. Next, by using a data structure introduced in the paper, the interval trie, previous solutions to the rectangle intersection and connected component problems are improved upon. Finally, a fast intersection finding algorithm for arbitrarily oriented line segments is presented.},
  langid = {english},
  annotation = {MAG ID: 2091082218},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\WULGG8IK\\Karlsson_Overmars_1988_Scanline algorithms on a grid.pdf}
}

@article{khosravaniSimpleAlgorithmConvex2013,
  title = {A Simple Algorithm for Convex Hull Determination in High Dimensions},
  author = {Khosravani, H. and Ruano, António E. and Ferreira, Pedro M.},
  date = {2013-11-07},
  pages = {109--114},
  doi = {10.1109/wisp.2013.6657492},
  abstract = {Selecting suitable data for neural network training, out of a larger set, is an important task. For approximation problems, as the role of the model is a nonlinear interpolator, the training data should cover the whole range where the model must be used, i.e., the samples belonging to the convex hull of the data should belong to the training set. Convex hull is also widely applied in reducing training data for SVM classification. The determination of the samples in the convex-hull of a set of high dimensions, however, is a time-complex task. In this paper, a simple algorithm for this problem is proposed.},
  langid = {english},
  annotation = {MAG ID: 2077428684},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\5IYLNT4B\\Khosravani et al_2013_A simple algorithm for convex hull determination in high dimensions.pdf}
}

@article{kingDangersExtremeCounterfactuals2006,
  title = {The {{Dangers}} of {{Extreme Counterfactuals}}},
  author = {King, Gary and Zeng, Langche},
  date = {2006-03-20},
  journaltitle = {Political Analysis},
  volume = {14},
  number = {2},
  pages = {131--159},
  doi = {10.1093/pan/mpj004},
  abstract = {We address the problem that occurs when inferences about counterfactuals—predictions, ‘‘what-if’’ questions, and causal effects—are attempted far from the available data. The danger of these extreme counterfactuals is that substantive conclusions drawn from statistical models that fit the data well turn out to be based largely on speculation hidden in convenient modeling assumptions that few would be willing to defend. Yet existing statistical strategies provide few reliable means of identifying extreme counterfactuals. We offer a proof that inferences farther from the data allow more model dependence and then develop easyto-apply methods to evaluate how model dependent our answers would be to specified counterfactuals. These methods require neither sensitivity testing over specified classes of models nor evaluating any specific modeling assumptions. If an analysis fails the simple tests we offer, then we know that substantive results are sensitive to at least some modeling choices that are not based on empirical evidence. Free software that accompanies this article implements all the methods developed.},
  langid = {english},
  annotation = {MAG ID: 2121275048},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\L7UZW6V7\\King_Zeng_2006_The Dangers of Extreme Counterfactuals.pdf}
}

@article{kleskConstructionNeurofuzzyNetwork2008,
  title = {Construction of a {{Neurofuzzy Network Capable}} of {{Extrapolating}} (and {{Interpolating}}) {{With Respect}} to the {{Convex Hull}} of a {{Set}} of {{Input Samples}} in \$\{\{\textbackslash bb \vphantom{\}\}}{{R}}\vphantom\{\}\vphantom\{\}\^n\$},
  author = {Klesk, P.},
  date = {2008-10-01},
  journaltitle = {IEEE Transactions on Fuzzy Systems},
  volume = {16},
  number = {5},
  pages = {1161--1179},
  doi = {10.1109/tfuzz.2008.924337},
  abstract = {The problem of regression estimation is considered with a specific regard for the distinction between interpolation and extrapolation. A neurofuzzy network named NFECH is proposed that is capable of extrapolating (and interpolating) with respect to the convex hull of a finite set of input samples X sub Ropfn. The geometrical construction of the proposed network is explained both mathematically and graphically. The illustrations explain how the particular parts of the construction work, and also show the final surfaces of the obtained models. The method is tested on artificial datasets generated from mathematical functions according to various statistical distributions. Also, comparisons to the commonly used radial basis function (RBF), multilayered perceptron (MLP) neural networks, and to fuzzy rule interpretation (FRI)/fuzzy rule extrapolation (FRE) approach are presented.},
  langid = {english},
  annotation = {MAG ID: 1977863160},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\CIMJM9VV\\Klesk_2008_Construction of a Neurofuzzy Network Capable of Extrapolating (and.pdf}
}

@article{lclarksonkennethApplicationsRandomSampling1989,
  title = {Applications of Random Sampling in Computational Geometry Ii},
  author = {{L ClarksonKenneth} and {W ShorPeter}},
  date = {1989},
  journaltitle = {Discrete and Computational Geometry},
  abstract = {We use random sampling for several new geometric algorithms. The algorithms are "Las Vegas," and their expected bounds are with respect to the random behavior of the algorithms. These algorithms fo...},
  langid = {english},
  annotation = {MAG ID: 3004480822}
}

@article{lohExtrapolationErrorsLinear2007,
  title = {Extrapolation Errors in Linear Model Trees},
  author = {Loh, Wei-Yin and Chen, Chien-Wei and Zheng, Wei},
  date = {2007-08-01},
  journaltitle = {ACM Transactions on Knowledge Discovery From Data},
  volume = {1},
  number = {2},
  pages = {6},
  doi = {10.1145/1267066.1267067},
  abstract = {Prediction errors from a linear model tend to be larger when extrapolation is involved, particularly when the model is wrong. This article considers the problem of extrapolation and interpolation errors when a linear model tree is used for prediction. It proposes several ways to curtail the size of the errors, and uses a large collection of real datasets to demonstrate that the solutions are effective in reducing the average mean squared prediction error. The article also provides a proof that, if a linear model is correct, the proposed solutions have no undesirable effects as the training sample size tends to infinity.},
  langid = {english},
  annotation = {MAG ID: 1997878053},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\Z2563IGS\\Loh et al_2007_Extrapolation errors in linear model trees.pdf}
}

@article{lopezEffectiveParametrizationAsymptotic2000,
  title = {An Effective Parametrization for Asymptotic Extrapolations},
  author = {Lopez, S.},
  date = {2000-08-18},
  journaltitle = {Computer Methods in Applied Mechanics and Engineering},
  volume = {189},
  number = {1},
  pages = {297--311},
  doi = {10.1016/s0045-7825(99)00297-2},
  abstract = {Abstract   When considering the range of validity of asymptotic expansions, the choice of the expansion parameter plays an essential role. In effect, with the exception of a posteriori adjustments of the expansion components, the definition of this parameter is the only way of improving the approximation at a finite distance from the initial point. Good parametrization equations are usually obtained by norms that involve all variables of the problem or based on the nature of the mechanical phenomena. Here this equation is constructed following the criteria of reducing the residual error and at the same time exactly reproducing solutions that admit a polynomial second degree representation. That choice is automatic in the analysis and improves the domain of accurate approximations. An appropriate predictor–corrector scheme was implemented to compare extrapolations obtained by this and several standard parametrizations. The solution scheme was used to analyse geometrically nonlinear plane frame structures.},
  langid = {english},
  annotation = {MAG ID: 2100182975},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\ER7EAZ9S\\Lopez_2000_An effective parametrization for asymptotic extrapolations.pdf}
}

@article{markouNoveltyDetectionReview2003,
  title = {Novelty Detection: A Review—Part 1: Statistical Approaches},
  author = {Markou, M. and Singh, Sameer},
  date = {2003-12-01},
  journaltitle = {Signal Processing},
  volume = {83},
  number = {12},
  pages = {2481--2497},
  doi = {10.1016/j.sigpro.2003.07.018},
  abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. Novelty detection is one of the fundamental requirements of a good classification or identification system since sometimes the test data contains information about objects that were not known at the time of training the model. In this paper we provide state-of-the-art review in the area of novelty detection based on statistical approaches. The second part paper details novelty detection using neural networks. As discussed, there are a multitude of applications where novelty detection is extremely important including signal processing, computer vision, pattern recognition, data mining, and robotics.},
  langid = {english},
  annotation = {MAG ID: 2095345875},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\6FV3PFA9\\Markou_Singh_2003_Novelty detection.pdf}
}

@article{markouNoveltyDetectionReview2003a,
  title = {Novelty Detection: A Review—Part 2: Neural Network Based Approaches},
  author = {Markou, M. and Singh, Sameer},
  date = {2003-12-01},
  journaltitle = {Signal Processing},
  volume = {83},
  number = {12},
  pages = {2499--2521},
  doi = {10.1016/j.sigpro.2003.07.019},
  abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network-based approaches for novelty detection. Statistical approaches are covered in Part 1 paper.},
  langid = {english},
  annotation = {MAG ID: 1499399937},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\JXKAW5HF\\Markou_Singh_2003_Novelty detection.pdf}
}

@article{matalasExtremesExtrapolationSurprise1999,
  title = {B. {{Extremes}}, {{Extrapolation}}, {{And Surprise}}},
  author = {Matalas, Nicholas and Bier, Vicki M.},
  date = {1999-02-01},
  journaltitle = {Risk Analysis},
  volume = {19},
  number = {1},
  pages = {49--54},
  doi = {10.1111/j.1539-6924.1999.tb00387.x},
  langid = {english},
  annotation = {MAG ID: 2020327299},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\FE67LARX\\Matalas_Bier_1999_B.pdf}
}

@article{mcdanielConceptualBasisFunction2005,
  title = {The Conceptual Basis of Function Learning and Extrapolation: {{Comparison}} of Rule-Based and Associative-Based Models},
  author = {McDaniel, Mark A. and Busemeyer, Jerome R.},
  date = {2005-02-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  volume = {12},
  number = {1},
  eprint = {15948282},
  eprinttype = {pmid},
  pages = {24--42},
  doi = {10.3758/bf03196347},
  abstract = {The purpose of this article is to provide a foundation for a more formal, systematic, and integrative approach to function learning that parallels the existing progress in category learning. First, we note limitations of existing formal theories. Next, we develop several potential formal models of function learning, which include expansion of classic rule-based approaches and associative-based models. We specify for the first time psychologically based learning mechanisms for the rule models. We then present new, rigorous tests of these competing models that take into account order of difficulty for learning different function forms and extrapolation performance. Critically, detailed learning performance was also used to conduct the model evaluations. The results favor a hybrid model that combines associative learning of trained input—prediction pairs with a rule-based output response for extrapolation (EXAM).},
  langid = {english},
  annotation = {MAG ID: 2069082406},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\FPWF2NM5\\McDaniel_Busemeyer_2005_The conceptual basis of function learning and extrapolation.pdf}
}

@article{montefuscoAlgorithm677C11989,
  title = {Algorithm 677 {{C1}} Surface Interpolation},
  author = {Montefusco, L.B. and Casciola, Giulio},
  date = {1989-12-01},
  journaltitle = {ACM Transactions on Mathematical Software},
  volume = {15},
  number = {4},
  pages = {365--374},
  doi = {10.1145/76909.76914},
  abstract = {A method of bivariate interpolation and smooth surface fitting is developed for rapidly varying Z values given at points irregularly distributed in the x-y plane. The surface is constructed by means of C 1  triangular interpolants defined on a triangulation of the convex hull of the points set. The needed partial derivative values are estimated by a new method based on a minimization criterion making use of a tension parameter},
  langid = {english},
  annotation = {MAG ID: 1994120908},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\HRY9HKPY\\Montefusco_Casciola_1989_Algorithm 677 C1 surface interpolation.pdf}
}

@article{montgomeryIntroductionLinearRegression1981,
  title = {Introduction to {{Linear Regression Analysis}}},
  author = {Montgomery, Douglas C. and {J. M. Cass} and Peck, Elizabeth A. and Vining, G. Geoffrey},
  date = {1981-12-01},
  journaltitle = {Journal of The Royal Statistical Society Series C-applied Statistics},
  doi = {10.2307/2348054},
  abstract = {Introduction to Linear Regression Analysis. By D. C. Montgomery and E. A. Peck. New York and Chichester, Wiley, 1982. 504 p. 23.5 cm. £25.77.},
  langid = {english},
  annotation = {MAG ID: 2797532987},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\8H4K7IHW\\Montgomery et al_1981_Introduction to Linear Regression Analysis.pdf}
}

@article{moreiraConcaveHullKnearest2007,
  title = {Concave Hull: A k-Nearest Neighbours Approach for the Computation of the Region Occupied by a Set of Points},
  author = {Moreira, Adriano and Santos, Maribel Yasmina},
  date = {2007-03-08},
  pages = {61--68},
  abstract = {This paper describes an algorithm to compute the envelope of a set of points in a plane, which generates convex or non-convex hulls that represent the area occupied by the given points. The proposed algorithm is based on a k-nearest neighbours approach, where the value of k, the only algorithm parameter, is used to control the “smoothness” of the final solution. The obtained results show that this algorithm is able to deal with arbitrary sets of points, and that the time to compute the polygons increases approximately linearly with the number of points.},
  langid = {english},
  annotation = {MAG ID: 1574059304}
}

@article{nichollXYConvexHull1983,
  title = {On the {{X-Y}} Convex Hull of a Set of {{X-Y}} Polygons},
  author = {Nicholl, Tina M. and Lee, Der-Tsai and Liao, Yuh-Zen and Wong, Chak-Kuen},
  date = {1983-12-01},
  journaltitle = {Bit Numerical Mathematics},
  volume = {23},
  number = {4},
  pages = {456--471},
  doi = {10.1007/bf01933620},
  abstract = {We study the class of rectilinear polygons, calledX – Y polygons, with horizontal and vertical edges, which are frequently used as building blocks for very large-scale integrated (VLSI) circuit layout and wiring. In the paper we introduce the notion of convexity within the class ofX – Y polygons and present efficient algorithms for computing theX – Y convex hulls of anX – Y polygon and of a set ofX – Y polygons under various conditions. Unlike convex hulls in the Euclidean plane, theX – Y convex hull of a set ofX – Y polygons may not exist. The condition under which theX – Y convex hull exists is given and an algorithm for testing if the given set ofX – Y polygons satisfies the condition is also presented.},
  langid = {english},
  annotation = {MAG ID: 2070165907},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\BVKABUGI\\Nicholl et al_1983_On the X-Y convex hull of a set of X-Y polygons.pdf}
}

@article{ottmannDefinitionComputationRectilinear1984,
  title = {On the Definition and Computation of Rectilinear Convex Hulls},
  author = {Ottmann, Thomas and Soisalon-Soininen, Eljas and Wood, Derick},
  date = {1984-09-01},
  journaltitle = {Information Sciences},
  volume = {33},
  number = {3},
  pages = {157--171},
  doi = {10.1016/0020-0255(84)90025-2},
  abstract = {Abstract   Recently the computation of the rectilinear convex hull of a collection of rectilinear polygons has been studied by a number of authors. From these studies three distinct definitions of rectilinear convex hulls have emerged. We examine these three definitions for point sets in general, pointing out some of their consequences, and we give optimal algorithms to compute the corresponding rectilinear convex hulls of a finite set of points in the plane.},
  langid = {english},
  annotation = {MAG ID: 1966720998},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\5NH9ALE4\\Ottmann et al_1984_On the definition and computation of rectilinear convex hulls.pdf}
}

@article{parkNewConcaveHull2012,
  title = {A {{New Concave Hull Algorithm}} and {{Concaveness Measure}} for N-Dimensional {{Datasets}}},
  author = {Park, Jin-Seo and Oh, Se-Jong},
  date = {2012-05-01},
  journaltitle = {Journal of Information Science and Engineering},
  volume = {28},
  number = {3},
  pages = {587--600},
  doi = {10.6688/jise.2012.28.3.10},
  abstract = {Convex and concave hulls are useful concepts for a wide variety of application areas, such as pattern recognition, image processing, statistics, and classification tasks. Concave hull performs better than convex hull, but it is difficult to formulate and few algorithms are suggested. Especially, an n-dimensional concave hull is more difficult than a 2- or 3-dimensional one. In this paper, we propose a new concave hull algorithm for n-dimensional datasets. It is simple but creative. We show its application to dataset analysis. We also suggest a concaveness measure and a graph that captures geometric shape of an n-dimensional dataset. Proposed concave hull algorithm and concaveness measure/graph are implemented using java, and are posted to http://user.dankook.ac.kr/\textasciitilde bitl/dkuCH.},
  langid = {english},
  annotation = {MAG ID: 1515031947}
}

@article{patelLinearProgramDetect1995,
  title = {A Linear Program to Detect Extrapolation in Predicting New Responses of a Multiple Linear Regression Model},
  author = {Patel, Minnie H.},
  date = {1995-10-01},
  journaltitle = {Computers \& Industrial Engineering},
  volume = {28},
  number = {4},
  pages = {787--791},
  doi = {10.1016/0360-8352(95)00013-q},
  abstract = {A region of interpolation is defined as the smallest convex set containing all original n data points used to build a regression model. In this paper, we present a linear program with n variables and (k + 1) constraints whose feasibility exactly determines whether or not a given new point, at which a response is predicted, is an extrapolation. Here k is the number of regressor variables used to build the regression model. This method has an advantage over the other methods used in the literature for the determination of extrapolation, in that, whenever a new point is indeed an extrapolation point, the developed method identifies it as an extrapolation, while the other methods may fail to identify it as an extrapolation point.},
  langid = {english},
  annotation = {MAG ID: 2075342411},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\TI8WZQYG\\Patel_1995_A linear program to detect extrapolation in predicting new responses of a.pdf}
}

@article{porterIntroductionLinearRegression1993,
  title = {Introduction to {{Linear Regression Analysis}}},
  author = {Porter, Donald R. and Montgomery, Douglas C. and Peck, Elizabeth A. and Vining, G. Geoffrey},
  date = {1993-05-01},
  volume = {25},
  number = {2},
  pages = {224--225},
  abstract = {Preface. Introduction. Simple Linear Regression. Multiple Linear Regression. Model Adequacy Checking. Transformations and Weighting to Correct Model Inadequacies. Diagnostics for Leverage and Influence. Polynomial Regression Models. Indicator Variables. Variable Selection and Model Building. Multicollinearity. Robust Regression. Introduction to Nonlinear Regression. Generalized Linear Models. Other Topics in the Use of Regression Analysis. Validation of Regression Models. Appendix A. Statistical Tables. Appendix B. Data Sets for Exercises. Appendix C. Supplemental Technical Material. References. Index.},
  langid = {english},
  annotation = {MAG ID: 2162027833}
}

@article{rawlinsOptimalComputationFinitely1987,
  title = {Optimal Computation of Finitely Oriented Convex Hulls},
  author = {Rawlins, Gregory J. E. and Wood, Derick},
  date = {1987-02-01},
  journaltitle = {Information \& Computation},
  volume = {72},
  number = {2},
  pages = {150--166},
  doi = {10.1016/0890-5401(87)90045-9},
  abstract = {Abstract   We define four versions of the “convex hull” of a simple finitely oriented polygon (i.e., a polygon whose edge orientations all belong to some fixed finite set of angles) and give optimal algorithms to find them. Two of these generalize the notions of the orthogonal convex hull of an orthogonal polygon and the traditional “bounding box” of a polygon. Three of the hulls have worst-case time complexity Θ(n + f) and worst case space complexity θ(n) space, where n is the number of edges of a given polygon and f (≥2) is the number of allowed orientations. We also show that testing whether an arbitrary simple polygon is (finitely oriented) convex has worst-case time and space complexity θ(n + f) and θ(n), respectively.},
  langid = {english},
  annotation = {MAG ID: 1966582767},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\LUUBKFP7\\Rawlins_Wood_1987_Optimal computation of finitely oriented convex hulls.pdf}
}

@article{rejerHypertubePossibleInterpolation2006,
  title = {A {{Hypertube}} as a {{Possible Interpolation Region}} of a {{Neural Model}}},
  author = {Rejer, Izabela and Mikolajczyk, Marek},
  date = {2006-01-01},
  journaltitle = {Lecture Notes in Computer Science},
  pages = {123--132},
  abstract = {The aim of this article is to present a method which can be applied to determine interpolation region of a multidimensional neural model. The method is based on the parametric curve modelling. The idea of it is to surround the parametric curve model with the hypertube covering most of the data points used in a neural model training. The practical application of the method will be shown via a system of an unemployment rate in Poland in years 1992-1999.},
  langid = {english},
  annotation = {MAG ID: 2884472321}
}

@article{sidiPracticalExtrapolationMethods2003,
  title = {Practical {{Extrapolation Methods}}: {{Theory}} and {{Applications}}},
  author = {Sidi, Avram},
  date = {2003-06-05},
  abstract = {An important problem that arises in many scientific and engineering applications is that of approximating limits of infinite sequences which in most instances converge very slowly. Thus, to approximate limits with reasonable accuracy, it is necessary to compute a large number of terms, and this is in general costly. These limits can be approximated economically and with high accuracy by applying suitable extrapolation (or convergence acceleration) methods to a small number of terms. This book is concerned with the coherent treatment, including derivation, analysis, and applications, of the most useful scalar extrapolation methods. The methods it discusses are geared toward problems that commonly arise in scientific and engineering disciplines. It differs from existing books on the subject in that it concentrates on the most powerful nonlinear methods, presents in-depth treatments of them, and shows which methods are most effective for different classes of practical nontrivial problems; it also shows how to fine-tune these methods to obtain the best numerical results. This state-of-the-art reference on the theory and practice of extrapolation methods will interest mathematicians interested in the theory of the relevant methods as well as giving applied scientists and engineers a practical guide to applying speed-up methods in the solution of difficult computational problems. Avram Sidi is Professor is Numerical Analysis in the Computer Science Department at the Technion-Israel Institute of Technology and holds the Technion Administration Chair in Computer Science. He has published extensively in various areas of numerical analysis and approximation theory and in journals such as Mathematics of Computation, SIAM Review, SIAM Journal on Numerical Analysis, Journal of Approximation Theory, Journal of Computational and Applied Mathematics, Numerische Mathematik, and Journal of Scientific Computing. Professor Sidi's work has involved the development of novel methods, their detailed mathematical analysis, design of efficient algorithms for their implementation, and their application to difficult practical problems. His methods and algorithms are successfully used in various scientific and engineering disciplines.},
  langid = {english},
  annotation = {MAG ID: 1556398709}
}

@article{silvermanMinimumCoveringEllipses1980,
  title = {Minimum {{Covering Ellipses}}},
  author = {Silverman, Bernard W. and Titterington, D. M.},
  date = {1980-12-01},
  journaltitle = {Siam Journal on Scientific and Statistical Computing},
  volume = {1},
  number = {4},
  pages = {401--409},
  doi = {10.1137/0901028},
  abstract = {With the aid of a duality relation originally obtained in the theory of statistical experimental design, an exact terminating algorithm is developed for finding the ellipse of smallest area covering a given plane point set. Some applications and related problems are discussed. Empirical timings show the algorithm to be highly efficient, particularly for large sets of points.},
  langid = {english},
  annotation = {MAG ID: 1984432065},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\Z7NMIDFK\\Silverman_Titterington_1980_Minimum Covering Ellipses.pdf}
}

@article{suComparisonSequentialDelaunay1995,
  title = {A Comparison of Sequential {{Delaunay}} Triangulation Algorithms},
  author = {Su, Peter and Drysdale, Robert L. Scot},
  date = {1995-09-01},
  pages = {61--70},
  doi = {10.1145/220279.220286},
  abstract = {This paper presents an experimental comparison of a number of different algorithms for computing the Deluanay triangulation. The algorithms examined are: Dwyer’s divide and conquer algorithm, Fortune’s sweepline algorithm, several versions of the incremental algorithm (including one by Ohya, Iri, and Murota, a new bucketing-based algorithm described in this paper, and Devillers’s version of a Delaunay-tree based algorithm that appears in LEDA), an algorithm that incrementally adds a correct Delaunay triangle adjacent to a current triangle in a manner similar to gift wrapping algorithms for convex hulls, and Barber’s convex hull based algorithm. Most of the algorithms examined are designed for good performance on uniformly distributed sites. However, we also test implementations of these algorithms on a number of non-uniform distibutions. The experiments go beyond measuring total running time, which tends to be machine-dependent. We also analyze the major high-level primitives that algorithms use and do an experimental analysis of how often implementations of these algorithms perform each operation.},
  langid = {english},
  annotation = {MAG ID: 2033824339},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\BR4AVYZW\\Su_Drysdale_1995_A comparison of sequential Delaunay triangulation algorithms.pdf}
}

@article{tangSMI2012Full2012,
  title = {{{SMI}} 2012: {{Full GPU}} Accelerated Convex Hull Computation},
  author = {Tang, Min and Zhao, Jieyi and Tong, Ruofeng and Manocha, Dinesh},
  date = {2012-08-01},
  journaltitle = {Computers \& Graphics},
  volume = {36},
  number = {5},
  pages = {498--506},
  doi = {10.1016/j.cag.2012.03.015},
  abstract = {We present a hybrid algorithm to compute the convex hull of points in three or higher dimensional spaces. Our formulation uses a GPU-based interior point filter to cull away many of the points that do not lie on the boundary. The convex hull of remaining points is computed on a CPU. The GPU-based filter proceeds in an incremental manner and computes a pseudo-hull that is contained inside the convex hull of the original points. The pseudo-hull computation involves only localized operations and maps well to GPU architectures. Furthermore, the underlying approach extends to high dimensional point sets and deforming points. In practice, our culling filter can reduce the number of candidate points by two orders of magnitude. We have implemented the hybrid algorithm on commodity GPUs, and evaluated its performance on several large point sets. In practice, the GPU-based filtering algorithm can cull up to 85M interior points per second on an NVIDIA GeForce GTX 580 and the hybrid algorithm improves the overall performance of convex hull computation by 10-27 times (for static point sets) and 22-46 times (for deforming point sets).},
  langid = {english},
  annotation = {MAG ID: 184958601},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\7YRCRS6F\\Tang et al_2012_SMI 2012.pdf}
}

@article{titteringtonEstimationCorrelationCoefficients1978,
  title = {Estimation of {{Correlation Coefficients}} by {{Ellipsoidal Trimming}}},
  author = {Titterington, D. M. and Titterington, D. M.},
  date = {1978-11-01},
  journaltitle = {Applied statistics},
  volume = {27},
  number = {3},
  pages = {227--234},
  doi = {10.2307/2347157},
  abstract = {The ellipsoid of minimal content containing a set of points is used as a trimming device and as a direct means of estimating correlation coefficients for mid‐truncated data. Numerical results are presented for simulated data and for Fisher's data from Iris setosa plants.},
  langid = {english},
  annotation = {MAG ID: 2795487203},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\HBIHQKH8\\Titterington_Titterington_1978_Estimation of Correlation Coefficients by Ellipsoidal Trimming.pdf}
}

@article{tzengFindingConvexHulls2012,
  title = {Finding {{Convex Hulls Using Quickhull}} on the {{GPU}}},
  author = {Tzeng, Stanley and Owens, John D.},
  date = {2012-01-13},
  journaltitle = {arXiv: Computational Geometry},
  abstract = {Author(s): Tzeng, S; Owens, JD | Abstract: We present a convex hull algorithm that is accelerated on commodity graphics hardware. We analyze and identify the hurdles of writing a recursive divide and conquer algorithm on the GPU and divise a framework for representing this class of problems. Our framework transforms the recursive splitting step into a permutation step that is well-suited for graphics hardware. Our convex hull algorithm of choice is Quickhull. Our parallel Quickhull implementation (for both 2D and 3D cases) achieves an order of magnitude speedup over standard computational geometry libraries.},
  langid = {english},
  annotation = {MAG ID: 2158499120}
}

@article{vacavantReconstructionsNoisyDigital2017,
  title = {Reconstructions of {{Noisy Digital Contours}} with {{Maximal Primitives Based}} on {{Multi-Scale}}/{{Irregular Geometric Representation}} and {{Generalized Linear Programming}}},
  author = {Vacavant, Antoine and Kerautret, Bertrand and Roussillon, Tristan and Feschet, Fabien and {Fabien Feschet}},
  date = {2017-09-19},
  volume = {10502},
  pages = {291--303},
  doi = {10.1007/bfb0038202},
  abstract = {The reconstruction of noisy digital shapes is a complex question and a lot of contributions have been proposed to address this problem , including blurred segment decomposition or adaptive tangential covering for instance. In this article, we propose a novel approach combining multi-scale and irregular isothetic representations of the input contour, as an extension of a previous work [Vacavant et al., A Combined Multi-Scale/Irregular Algorithm for the Vectorization of Noisy Digital Contours , CVIU 2013]. Our new algorithm improves the representation of the contour by 1-D intervals, and achieves afterwards the decomposition of the contour into maximal arcs or segments. Our experiments with synthetic and real images show that our contribution can be employed as a relevant option for noisy shape reconstruction.},
  langid = {english},
  annotation = {MAG ID: 1646289414},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\IV95NLGY\\Vacavant et al_2017_Reconstructions of Noisy Digital Contours with Maximal Primitives Based on.pdf}
}

@article{verleysenLearningHighdimensionalData2001,
  title = {Learning High-Dimensional Data},
  author = {Verleysen, Michel},
  date = {2001-01-01},
  abstract = {Observations from real-world problems are often high-dimensional vectors, i.e. made up of many variables. Learning methods, including artificial neural networks, often have difficulties to handle a relatively small number of high-dimensional data. In this paper, we show how concepts gained from our intuition on 2- and 3-dimensional data can be misleading when used in high-dimensional settings. When then show how the "curse of dimensionality" and the "empty space phenomenon" can be taken into account in the design of neural network algorithms, and how non-linear dimension reduction techniques can be used to circumvent the problem. We conclude by an illustrative example of this last method on the forecasting of financial time series.},
  langid = {english},
  annotation = {MAG ID: 2105999656}
}

@article{wiensRobustPredictionExtrapolation2008,
  title = {Robust Prediction and Extrapolation Designs for Misspecified Generalized Linear Regression Models},
  author = {Wiens, Douglas P. and Xu, Xiaojian},
  date = {2008-01-01},
  journaltitle = {Journal of Statistical Planning and Inference},
  volume = {138},
  number = {1},
  pages = {30--46},
  doi = {10.1016/j.jspi.2007.05.025},
  abstract = {We study minimax robust designs for response prediction and extrapolation in biased linear regression models. We extend previous work of others by considering a nonlinear fitted regression response, by taking a rather general extrapolation space and, most significantly, by dropping all restrictions on the structure of the regressors. Several examples are discussed.},
  langid = {english},
  annotation = {MAG ID: 2105936640},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\LBY2LRPF\\Wiens_Xu_2008_Robust prediction and extrapolation designs for misspecified generalized linear.pdf}
}

@article{xiaBORDEREfficientComputation2006,
  title = {{{BORDER}}: Efficient Computation of Boundary Points},
  author = {Xia, Chenyi and Hsu, Wynne and Lee, Mong Li and Ooi, Beng Chin},
  date = {2006-03-01},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {18},
  number = {3},
  pages = {289--303},
  doi = {10.1109/tkde.2006.38},
  abstract = {This work addresses the problem of finding boundary points in multidimensional data sets. Boundary points are data points that are located at the margin of densely distributed data such as a cluster. We describe a novel approach called BORDER (a BOundaRy points DEtectoR) to detect such points. BORDER employs the state-of-the-art database technique - the Gorder kNN join and makes use of the special property of the reverse k nearest neighbor (RkNN). Experimental studies on data sets with varying characteristics indicate that BORDER is able to detect the boundary points effectively and efficiently.},
  langid = {english},
  annotation = {MAG ID: 2098920923},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\MU3XNW9Q\\Xia et al_2006_BORDER.pdf}
}

@article{xuConcaveHullAlgorithm2010,
  title = {A {{Concave Hull}} Algorithm for Scattered Data and Its Applications},
  author = {Xu, Junyi and Zheng, Zuoya and Feng, Yanping and Qing, Xihong},
  date = {2010-11-29},
  volume = {5},
  pages = {2430--2433},
  doi = {10.1109/cisp.2010.5648277},
  abstract = {In this paper, the novel methods and problems of Concave Hull are given through the scattered points set. The — Concave Hull algorithms, which simply connected region, are proposed based on Graham's scanning of Convex Hull. By the — Hull definition, the Convex Hull is the case when. The — Concave Hull algorithms and Graham's scanning are equivalent when. The experiment of the fault plane extraction of 3D seismic showed that the algorithm is an effective method for the extracting the fault plane problems under the appropriate conditions.},
  langid = {english},
  annotation = {MAG ID: 2016775478},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\KRI9BR4P\\Xu et al_2010_A Concave Hull algorithm for scattered data and its applications.pdf}
}

@article{zhongFindingConvexHull2014,
  title = {Finding Convex Hull Vertices in Metric Space},
  author = {Zhong, Jinhong and Tang, Ke and Qin, A. K.},
  date = {2014-07-06},
  pages = {1587--1592},
  doi = {10.1109/ijcnn.2014.6889699},
  abstract = {The convex hull has been extensively studied in computational geometry and its applications have spread over an impressive number of fields. How to find the convex hull is an important and challenging problem. Although many algorithms had been proposed for that, most of them can only tackle the problem in two or three dimensions and the biggest issue is that those algorithms rely on the samples' coordinates to find the convex hull. In this paper, we propose an approximation algorithm named FVDM, which only utilizes the information of the samples' distance matrix to find the convex hull. Experiments demonstrate that FVDM can effectively identify the vertices of the convex hull.},
  langid = {english},
  annotation = {MAG ID: 1994786644},
  file = {C\:\\Users\\obuli\\Zotero\\storage\\HE9KZET8\\Zhong et al_2014_Finding convex hull vertices in metric space.pdf}
}


